# -*- coding: utf-8 -*-
# ç›¸è¾ƒäº3çš„æ”¹è¿›ç‚¹ï¼šæ›´è¿›ä¸€æ­¥å¤„ç†ä¸åŒè§†é¢‘èŠ‚å¥ï¼Œå¯ä»¥åŠ ä¸€ä¸ªå˜åŒ–æ»‘çª—æœºåˆ¶åŠ¨æ€è°ƒæ•´ similarity_threshold
# å› ä¸ºä¸åŒè§†é¢‘ç‰‡æ®µä¸­çš„å†…å®¹å˜åŒ–é€Ÿç‡ä¸åŒï¼Œå¦‚æœç”¨ä¸€ä¸ªå›ºå®šçš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¦‚ 0.95ï¼‰ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹å¯èƒ½ä¼šæ¼æ‰å…³é”®å˜åŒ–ï¼Œè€Œåœ¨å¦ä¸€äº›åœºæ™¯ä¸‹åˆå¯èƒ½ä¿å­˜äº†å¤ªå¤šå†—ä½™å¸§
# ä½¿ç”¨äº†å¸§ç¼–å·ï¼ˆframe countï¼‰ä½œä¸ºå‘½åæ–¹å¼ï¼Œå¯ä»¥é€šè¿‡è§†é¢‘å¸§ç‡ï¼ˆfpsï¼‰è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼štimestamp_seconds = frame_count / fps  # fpsæ˜¯è§†é¢‘å¸§ç‡
import os
import cv2
import numpy as np
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
from collections import deque

# é…ç½®è·¯å¾„
video_path = "./videos/å¤§å­¦ç‰©ç†/å¤§å­¦ç‰©ç†ä¸Šphase4.mp4"
video_name = os.path.splitext(os.path.basename(video_path))[0]
output_dir = os.path.join(os.path.dirname(video_path), f"clip_candidates_{video_name}")
os.makedirs(output_dir, exist_ok=True)

# æ¨¡å‹åˆå§‹åŒ–
clip_model_name = "openai/clip-vit-base-patch16"
clip_model = CLIPModel.from_pretrained(clip_model_name)
clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model.to(device)

# å‚æ•°é…ç½®
frame_interval = 3
base_threshold = 0.95
min_threshold = 0.80
max_threshold = 0.98
sensitivity = 0.5  # æ§åˆ¶å¯¹æ³¢åŠ¨å˜åŒ–çš„å“åº”ç¨‹åº¦
window_size = 10   # æ»‘åŠ¨çª—å£å¤§å°ï¼ˆè®¡ç®—æœ€è¿‘Næ¬¡ç›¸ä¼¼åº¦æ³¢åŠ¨ï¼‰

# æ»‘åŠ¨çª—å£è®°å½•æœ€è¿‘çš„ç›¸ä¼¼åº¦
similarity_window = deque(maxlen=window_size)

# æå–å›¾åƒç‰¹å¾
def extract_features(image):
    inputs = clip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = clip_model.get_image_features(**inputs)
    return outputs / outputs.norm(p=2, dim=-1, keepdim=True)

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity(x, y):
    return torch.nn.functional.cosine_similarity(x, y).item()

# åŠ¨æ€è°ƒæ•´é˜ˆå€¼
def dynamic_threshold():
    if not similarity_window:
        return base_threshold
    avg = np.mean(similarity_window)
    std = np.std(similarity_window)
    # è¶Šæ³¢åŠ¨ï¼Œå½“å‰é˜ˆå€¼è¶Šé«˜ï¼ˆé˜²æ­¢å†—ä½™ï¼‰ï¼›è¶Šç¨³å®šï¼Œå½“å‰é˜ˆå€¼è¶Šä½ï¼ˆæ•æ‰å¾®å°å˜åŒ–ï¼‰
    adjusted = base_threshold - sensitivity * std
    return np.clip(adjusted, min_threshold, max_threshold)

# è§†é¢‘å¤„ç†
cap = cv2.VideoCapture(video_path)
frame_count = 0
saved_count = 0
last_feature = None

while True:
    ret, frame = cap.read()
    if not ret:
        break

    if frame_count % frame_interval == 0:
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb)
        feature = extract_features(pil_img)

        if last_feature is None:
            # é¦–å¸§ç›´æ¥ä¿å­˜
            save_path = os.path.join(output_dir, f"frame_{frame_count:06d}.jpg")
            pil_img.save(save_path)
            saved_count += 1
            last_feature = feature
        else:
            sim = cosine_similarity(last_feature, feature)
            similarity_window.append(sim)
            current_thresh = dynamic_threshold()

            if sim < current_thresh:
                save_path = os.path.join(output_dir, f"frame_{frame_count:06d}.jpg")
                pil_img.save(save_path)
                saved_count += 1
                last_feature = feature

    frame_count += 1

cap.release()
print(f"ğŸ¯ åŠ¨æ€ç­›é€‰å®Œæˆï¼Œå…±ä¿å­˜ {saved_count} å¼ å…³é”®å¸§ã€‚")
