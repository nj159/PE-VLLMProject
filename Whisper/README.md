
## 安装whisper
- conda create -n whisper python=3.10 -y
- conda activate whisper
- pip install -U openai-whisper
- pip install git+https://github.com/openai/whisper.git 
- pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git
- 参考：https://github.com/openai/whisper

### 安装ffmpeg
- sudo apt update
- sudo apt install ffmpeg

### 说话人分离工具安装
pip install pyannote.audio

pip install transformers==4.51.3 accelerate

## 将环境中的依赖保存为：pip freeze > requirements.txt
## 或者直接：
pip install --upgrade pip （更新pip)
pip install -r requirements.txt（即可安装所有依赖）

## 代码运行步骤
### clip_filter4.py
本模块实现了基于视觉语义变化的关键帧提取机制，旨在从教学视频中自动筛选出具有代表性的帧图像，以供后续分析使用。该方法依赖于 CLIP（Contrastive Language-Image Pretraining）模型对视频帧进行图像特征编码，并通过滑动窗口监控相邻帧之间的语义相似度变化，从而动态调整帧保存阈值，实现对显著视觉变化的响应。
视频按固定间隔采样（默认为每3帧处理一次），每帧图像通过 CLIP 模型提取特征，并与上一帧特征计算余弦相似度。系统维护一个滑动窗口记录近期的相似度序列，通过计算其均值与标准差来动态调整保存阈值：相似度波动越大，则阈值越高，以减少冗余帧；波动较小时则降低阈值，以捕捉细微变化。当当前帧与上一保存帧的相似度低于当前阈值时，视其为关键帧并予以保存。
最终，该模块输出一组以语义变化为基础筛选出的图像帧，为后续行为分析任务提供了信息浓缩的视觉输入。


### whisper_pyannote_image.py：为每个关键帧生成前后三秒和前后16秒对话上下文的csv文件（对话语言文本提取和说话人分离）
本模块旨在将提取自教学视频的关键帧与对应时段内的语音内容进行精确对齐，以构建结构化的“图像-文本-说话人”三模态数据集。系统结合 Whisper 模型的中文语音识别与 pyannote.audio 的说话人分离能力，自动标注关键帧在时间轴上对应的发言文本及其说话人身份。
首先，通过 `ffmpeg` 工具从原始视频中提取音频并统一采样格式；随后利用 Whisper 模型对音频进行分段式转录，得到精确的发言区间和文本内容。与此同时，借助 pyannote 的预训练说话人分离管线对整段音频进行发言人聚类，以实现文本与身份的对应。

为实现跨模态对齐，系统读取由第一阶段提取的关键帧图像及其时间戳，并设置双重时间窗口策略：±3秒窗口用于精确语境对齐，±16秒窗口用于构建宽语境摘要。系统逐帧遍历图像时间戳，对应提取该时段内的所有发言内容与发言者身份，进而生成用于图像语境建模的结构化文本串 `context_text` 与 `wide_context_text`。最终，所有数据整合为表格格式并导出为 CSV 文件，供后续图像分析与生成任务使用。
### summary_generator.py：读取一份包含图像编号和对话内容（前后三秒和前后16秒）的 CSV 文件，然后依次加载对应的图像，结合对话上下文，通过 Gemini 多模态模型生成每张图像的简短语境摘要（通过前后16秒的对话上下文），最后把这些摘要写回 CSV 文件中，支持断点续跑和失败重试。
该模块负责在图像与语音上下文对齐结果的基础上，为每一帧教学图像生成简明的语境摘要，以实现对视觉场景的语义浓缩表达。系统调用多模态大语言模型（如 Gemini）接口，将图像信息与前后语音对话内容联合输入，并输出一句话左右的语境概括文本。
首先，程序加载图像及其关联语音上下文（wide\_context\_text），并通过 base64 编码将图像嵌入到模型请求中。每次请求采用多轮对话格式，结合图像和文本构造多模态输入，提示模型输出“简洁总结图像所处语境”的短句。为增强稳健性，模块实现了指数退避的重试机制，以处理 API 请求失败等异常情况。
此外，模块支持断点续跑机制：如检测到已有输出文件，则自动跳过已生成摘要的记录，仅对尚未完成的样本进行处理。最终，每张关键帧图像均被赋予一条结构化的摘要信息，扩展了原始图像-语音对齐数据的语义维度。

### analysisimage.py用gemini分析image内容得到分析结果
本模块基于图像及其上下文摘要信息，调用多模态大语言模型（如 Gemini）对课堂场景中的教师非言语行为进行综合性分析，并以结构化 JSON 形式输出专业、具启发性的教学评估结果。其核心目标是提升图像数据的语义解释性，辅助教学行为的定性研究。
系统首先构建标准化提示模板（Prompt），其中融合了三类上下文信息：课程背景、图像摘要（即课堂情境）、以及图像前后语音对话内容。随后通过多轮对话接口与图像嵌入格式，向大语言模型提交图文联合分析请求。模型被明确要求输出包含以下三个维度的分析内容：① 非言语行为观察；② 教学意图与效果分析；③ 评价与建议。
此外，模块具备断点续跑能力，可跳过已完成分析的数据记录，并在图像缺失时自动记录错误状态，保证整体流程的鲁棒性。分析结果统一存储于 CSV 文件的 analysis_json 列中，为后续的可视化展示与质性研究提供结构化支持。

为实现面向教学视频的高质量图像行为分析，系统设计如下四阶段处理流程：
关键帧提取（代码段一）：使用 CLIP 模型提取视频帧图像特征，基于语义变化动态筛选具有代表性的关键帧图像。
图像-语音对齐（代码段二）：采用 Whisper 和 PyAnnote 模型对视频音频进行语音识别与说话人分离，并将结果与关键帧图像在时间轴上对齐，生成双重语音语境。
图像摘要生成（代码段三）：以图像及其宽时间窗口内的对话文本为输入，调用多模态大模型生成每张图像的语境摘要（summary_text），提升图像的语义表达能力。
教学行为分析（代码段四）：在上述摘要与对话信息的基础上，进一步调用大模型输出关于教学非言语行为的结构化分析结果（analysis_json），以支持教学质性研究与智能评估。


