import os
import subprocess
import torch
import cv2
import pandas as pd
from datetime import timedelta
from whisper import load_model
from pyannote.audio import Pipeline
from pyannote.core import Segment
from tqdm import tqdm

# === é…ç½®è·¯å¾„ ===
video_path = "./videos/å¤§å­¦ç‰©ç†/å¤§å­¦ç‰©ç†ä¸‹phase5.mp4"

# è‡ªåŠ¨æå–åç§°å¹¶æ„å»ºè·¯å¾„
video_dir = os.path.dirname(video_path)                # "./movies"
video_name = os.path.splitext(os.path.basename(video_path))[0]  # "ç”µå½±ç¾å­¦"

keyframe_dir = os.path.join(video_dir, f"clip_candidates_{video_name}")
output_csv_path = os.path.join(video_dir, f"{video_name}-è¯´è¯äººæ–‡æœ¬å›¾åƒ.csv")
tmp_wav_path = "/tmp/tmp_audio.wav"

# === æå–éŸ³é¢‘ï¼ˆwavï¼‰ ===
def extract_audio_ffmpeg(video_path, wav_path):
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-ac", "1", "-ar", "16000",
        "-vn", "-f", "wav", wav_path
    ]
    subprocess.run(cmd, check=True)

# === è·å–è§†é¢‘å¸§ç‡ ===
def get_video_fps(video_path):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    cap.release()
    return fps

# === åŠ è½½å…³é”®å¸§æ—¶é—´æˆ³ ===
def load_keyframe_timestamps(keyframe_dir, fps):
    timestamps = []
    for fname in sorted(os.listdir(keyframe_dir)):
        if fname.endswith(".jpg") and "frame_" in fname:
            frame_id = int(fname.split("_")[1].split(".")[0])
            time_sec = frame_id / fps
            timestamps.append((fname, time_sec))
    return timestamps

# === Whisper è¯­éŸ³è¯†åˆ« ===
def transcribe_whisper(wav_path):
    model = load_model("large")
    result = model.transcribe(wav_path, language="zh")
    return result["segments"]

# === Pyannote è¯´è¯äººåˆ†ç¦» ===
def diarize_speakers(wav_path):
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization")
    diarization = pipeline(wav_path)
    return diarization

# === ä¸»æµç¨‹ ===
def main():
    print("ğŸ” è·å–å¸§ç‡...")
    fps = get_video_fps(video_path)
    print(f"ğŸ è§†é¢‘å¸§ç‡: {fps:.2f} fps")

    print("ğŸ”Š æå–éŸ³é¢‘...")
    extract_audio_ffmpeg(video_path, tmp_wav_path)

    print("ğŸ“– Whisper è¯­éŸ³è¯†åˆ«ä¸­...")
    segments = transcribe_whisper(tmp_wav_path)

    print("ğŸ§‘â€ğŸ¤â€ğŸ§‘ Pyannote è¯´è¯äººåˆ†ç¦»ä¸­...")
    diarization = diarize_speakers(tmp_wav_path)

    print("ğŸ–¼ è¯»å–å…³é”®å¸§æ—¶é—´æˆ³...")
    keyframe_timestamps = load_keyframe_timestamps(keyframe_dir, fps)

    print("ğŸ“„ æ„å»ºå›¾åƒä¸»å¯¼çš„ä¸Šä¸‹æ–‡åŒ¹é…...")
    context_window = 3.0  # âœ… Â±3ç§’ä¸Šä¸‹æ–‡ï¼ˆç”¨äº context_textï¼‰
    wide_context_window = 16.0  # âœ… Â±16ç§’ä¸Šä¸‹æ–‡ï¼ˆç”¨äº wide_context_textï¼‰

    results = []

    for fname, ts in keyframe_timestamps:
        window_start = ts - context_window
        window_end = ts + context_window

        wide_start = ts - wide_context_window
        wide_end = ts + wide_context_window

        matched_segments = []
        matched_speakers = set()
        speaker_map = {}

        for seg in segments:
            seg_start = seg['start']
            seg_end = seg['end']

            # wide_context åŒ¹é…ï¼ˆä¸å½±å“åŸæœ‰ speaker åŒ¹é…ï¼‰
            in_context = max(seg_start, window_start) < min(seg_end, window_end)
            in_wide_context = max(seg_start, wide_start) < min(seg_end, wide_end)

            if in_context or in_wide_context:
                speaker_label = "æœªçŸ¥"
                for seg_track in diarization.itertracks(yield_label=True):
                    seg_t, _, label = seg_track
                    if max(seg_t.start, seg_start) < min(seg_t.end, seg_end):
                        speaker_label = label
                        break

                if speaker_label not in speaker_map:
                    speaker_map[speaker_label] = "æ•™å¸ˆ" if len(speaker_map) == 0 else "å­¦ç”Ÿ"
                speaker = speaker_map[speaker_label]

                if in_context:
                    matched_segments.append({
                        "speaker": speaker,
                        "text": seg['text'].strip()
                    })
                    matched_speakers.add(speaker)

        # æ„é€  context_textï¼ˆÂ±15ç§’ï¼‰å’Œ wide_context_textï¼ˆÂ±16ç§’ï¼‰
        context_text = " / ".join(f"{seg['speaker']}ï¼š{seg['text']}" for seg in matched_segments)

        wide_context_lines = []
        for seg in segments:
            seg_start = seg['start']
            seg_end = seg['end']
            if max(seg_start, wide_start) < min(seg_end, wide_end):
                # å°è¯•æ‰¾ speaker_label
                speaker_label = "æœªçŸ¥"
                for seg_track in diarization.itertracks(yield_label=True):
                    seg_t, _, label = seg_track
                    if max(seg_t.start, seg_start) < min(seg_t.end, seg_end):
                        speaker_label = label
                        break

                speaker = speaker_map.get(speaker_label, "æœªçŸ¥")
                wide_context_lines.append(f"{speaker}ï¼š{seg['text'].strip()}")

        wide_context_text = " / ".join(wide_context_lines)

        results.append({
            "image_id": fname,
            "timestamp": str(timedelta(seconds=int(ts))),
            "context_text": context_text,
            "wide_context_text": wide_context_text,
            "context_speakers": ",".join(sorted(matched_speakers))
        })

    print("ğŸ’¾ å†™å…¥ CSV æ–‡ä»¶...")
    df = pd.DataFrame(results)
    df.to_csv(output_csv_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å®Œæˆ: å›¾åƒä¸Šä¸‹æ–‡ç»“æœä¿å­˜è‡³ {output_csv_path}")


if __name__ == "__main__":
    main()
